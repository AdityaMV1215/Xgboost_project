{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting(XGBoost)\n",
    "***\n",
    "\n",
    "Welcome to the Extreme Gradient Boosting Project!\n",
    "\n",
    "**This is Extreme Gradient Boosting Project**\n",
    "\n",
    "In this project, you will demonstrate what you have learned in this course by conducting an experiment dealing with Loan Prediction.\n",
    "\n",
    "We have seen in the lectures How Extreme Gradient Boosting works.\n",
    "\n",
    "## What we have learned so far:\n",
    "*\n",
    "*\n",
    "*\n",
    "\n",
    "## What we are going to do?\n",
    "* You have clean data-set. We will use an approach similar to previous grid search but will divide the parmeter in two parts.\n",
    "* Choose default values for Xgboost Classifier.\n",
    "* Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree.\n",
    "\n",
    "## What your will learn by doing this assignment ?\n",
    "\n",
    "\n",
    "* You will learn to build Xgboost model.\n",
    "\n",
    "### Dataset\n",
    "To perform Logistic Regression task we will use `Loan Prediction` dataset.  \n",
    "\n",
    "This dataset contains following features: \n",
    "- ApplicantIncome\n",
    "- CoapplicantIncome\n",
    "- Loan Amount\n",
    "- Loan Amount term\n",
    "- Credit History\n",
    "- Property_Area\n",
    "- Self_Employed\n",
    "- Education\n",
    "- Dependents\n",
    "- Married\n",
    "- Gender\n",
    "- Loan_ID\n",
    "\n",
    "**Target Variable:**\n",
    "- Loan Status\n",
    "\n",
    "**Details information is mentioned in each task.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\greyatom2.7\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = pd.read_csv('../Data/loan_clean_data.csv')\n",
    "# split data into X and y\n",
    "X = dataset.iloc[:,:-1]\n",
    "y = dataset.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start Extreme Gradient Boosting with its Parameter tune\n",
    "\n",
    "* Extreme Gradient Boosting has lot parameter to tune, but we will be touching some of it.\n",
    "* We have divided the parameter for the ease.\n",
    "\n",
    "## Write a function `myXGBoost` that:\n",
    "* Will take following param_grid along with model, dataset, KFold that will fit a model and will return the accuracy and best_params.\n",
    "* You will using GridSearchCV.\n",
    "* You will be using ***kwargs* (To set parameters to the base classifier)\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "| Parameter | dtype | argument type | default value | description |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| X_train | DataFrame | compulsory | | Dataframe containing feature variables for training|\n",
    "| X_test | DataFrame | compulsory | | Dataframe containing feature variables for testing|\n",
    "| y_train | Series/DataFrame | compulsory | | Training dataset target Variable |\n",
    "| y_test | Series/DataFrame | compulsory | | Testing dataset target Variable |\n",
    "| model | int | compulsory | | Which model needs to be build |\n",
    "| param_grid | Dict | compulsory | | Dictionary of parameter |\n",
    "| KFold | int | optiional | 3 | For Kfold validation |\n",
    "| **kwargs |  | compulsory | | additional parameter to be given |\n",
    "\n",
    "### Return :\n",
    "\n",
    "| Return | dtype | description |\n",
    "| --- | --- | --- |\n",
    "| accuracy | float | accuracy of model using those params |\n",
    "| best_params | Dict | Dictionary of best fit parameter the model  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\"max_depth\": [2, 3, 4, 5, 6, 7, 9, 11],\n",
    "             \"min_child_weight\": [4, 6, 7, 8],\n",
    "             \"subsample\": [0.6, .7, .8, .9, 1],\n",
    "             \"colsample_bytree\": [0.6, .7, .8, .9, 1]\n",
    "             }\n",
    "\n",
    "param_grid2 = {\"gamma\": [0, 0.05, 0.1, 0.3, 0.7, 0.9, 1],\n",
    "             \"reg_alpha\": [0, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "             \"reg_lambda\": [0.05, 0.1, 0.5, 1.0]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myXGBoost(X_train, X_test, y_train, model, param_grid, KFold=3, **kwargs):\n",
    "    \n",
    "    if kwargs:\n",
    "        model.set_params(**kwargs)\n",
    "    gs_cv = GridSearchCV(model, param_grid=param_grid, cv=KFold, verbose=0)\n",
    "    gs_cv.fit(X_train, y_train)\n",
    "    best_params = gs_cv.best_params_\n",
    "    y_pred = gs_cv.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    \n",
    "    return accuracy, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(seed=9)\n",
    "accuracy, best_params = myXGBoost(X_train, X_test, y_train, xgb, param_grid1, KFold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79670329670329665,\n",
       " {'colsample_bytree': 0.7,\n",
       "  'gamma': 0.9,\n",
       "  'max_depth': 2,\n",
       "  'min_child_weight': 6,\n",
       "  'subsample': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Continue with Extreme Gradient Boosting  Parameter tuning\n",
    "\n",
    "* Now we have tunned the first few parameter, now we use them and tune the rest params.\n",
    "\n",
    "## Write a function `param2` that:\n",
    "* Will take following param_grid along with model, dataset that will use **myXGBoost** and will return the accuracy and best_params.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "| Parameter | dtype | argument type | default value | description |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| X_train | DataFrame | compulsory | | Dataframe containing feature variables for training|\n",
    "| X_test | DataFrame | compulsory | | Dataframe containing feature variables for testing|\n",
    "| y_train | Series/DataFrame | compulsory | | Training dataset target Variable |\n",
    "| y_test | Series/DataFrame | compulsory | | Testing dataset target Variable |\n",
    "| model | int | compulsory | | Which model needs to be build |\n",
    "| param_grid | Dict | compulsory | | Dictionary of parameter |\n",
    "\n",
    "### Return :\n",
    "\n",
    "| Return | dtype | description |\n",
    "| --- | --- | --- |\n",
    "| accuracy | float | accuracy of model using those params |\n",
    "| best_params | Dict | Dictionary of best fit parameter the model  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb1 = XGBClassifier(seed=9,colsample_bytree=0.7,gamma= 0.9,max_depth=2,min_child_weight=6,subsample=1)\n",
    "def param2(X_train, X_test, y_train, model, param_grid2):\n",
    "    return myXGBoost(X_train, X_test, y_train, model, param_grid2,\n",
    "                    colsample_bytree=0.7,gamma= 0.9,max_depth=2,min_child_weight=6,subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1, best_params1 = param2(X_train, X_test, y_train, xgb, param_grid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79670329670329665, {'n_estimators': 20, 'reg_alpha': 0, 'reg_lambda': 0.05})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy1, best_params1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Xgboost using the bestmodel\n",
    "\n",
    "* Now we have tunned the parameter, we gonna use them in the model.\n",
    "\n",
    "## Write a function `xgboost` that:\n",
    "* Will take following dataset and will return the accuracy and best_params.\n",
    "\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "| Parameter | dtype | argument type | default value | description |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| X_train | DataFrame | compulsory | | Dataframe containing feature variables for training|\n",
    "| X_test | DataFrame | compulsory | | Dataframe containing feature variables for testing|\n",
    "| y_train | Series/DataFrame | compulsory | | Training dataset target Variable |\n",
    "| y_test | Series/DataFrame | compulsory | | Testing dataset target Variable |\n",
    "| **kwargs |  | compulsory | | additional parameter to be given |\n",
    "\n",
    "### Return :\n",
    "\n",
    "| Return | dtype | description |\n",
    "| --- | --- | --- |\n",
    "| accuracy | float | accuracy of model using those params |\n",
    "\n",
    "To-Do list :\n",
    "\n",
    "Check for different n_estimators and learning_rate whether the score are varing and find the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, **kwargs):\n",
    "    xgb1 = XGBClassifier(seed=9)\n",
    "    if kwargs:\n",
    "        xgb1.set_params(**kwargs)\n",
    "    xgb1.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = xgb1.predict(X_test)\n",
    "    return accuracy_score(y_pred, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79670329670329665"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score = xgboost(X_train, X_test, y_train,colsample_bytree=0.7,gamma= 0.9,max_depth=2,min_child_weight=6,subsample=1,n_estimators=10,\n",
    "                       reg_alpha=0, reg_lambda=0.05,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
